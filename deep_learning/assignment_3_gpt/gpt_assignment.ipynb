{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HQug57ffC1v"
   },
   "source": [
    "# Assignment 3 - Autoregressive Language Modeling with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211602,
     "status": "ok",
     "timestamp": 1769512765964,
     "user": {
      "displayName": "Amer Delic",
      "userId": "17988139909285196208"
     },
     "user_tz": -60
    },
    "id": "mVXOyydFfC1w",
    "outputId": "10495be2-78aa-41ee-b4da-c9dde20cc8ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: datasets==3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (4.15.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (80.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (23.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (1.3.4)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets==3.1.0) (6.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==3.1.0) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (0.21.1)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.23.0->datasets==3.1.0) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.23.0->datasets==3.1.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.23.0->datasets==3.1.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.23.0->datasets==3.1.0) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.1.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.1.0) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.23.0->datasets==3.1.0) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch==2.4.1) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets==3.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets==3.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets==3.1.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.1.0) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer-slim->huggingface-hub>=0.23.0->datasets==3.1.0) (8.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.4.1 datasets==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1769513205720,
     "user": {
      "displayName": "Amer Delic",
      "userId": "17988139909285196208"
     },
     "user_tz": -60
    },
    "id": "zXF-uhEjfC1x"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31y0dQCofC1x"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1769513207821,
     "user": {
      "displayName": "Amer Delic",
      "userId": "17988139909285196208"
     },
     "user_tz": -60
    },
    "id": "Grb7vq3rfC1x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "class MovieDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, block_size=256):\n",
    "        # Load dataset\n",
    "        ds = load_dataset(\"Pablinho/movies-dataset\")\n",
    "        data = ds['train'].to_pandas()\n",
    "\n",
    "        # Convert to pandas and create string format\n",
    "        text_data = \"\"\n",
    "        for _, row in data.iterrows():\n",
    "            text_data += f\"{row['Title']}: {row['Overview']}\\n\"\n",
    "\n",
    "        # Create character mappings\n",
    "        chars = sorted(list(set(text_data)))\n",
    "        self.string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "        # Encode text to integers\n",
    "        encoded_data = [self.string_to_int[c] for c in text_data]\n",
    "\n",
    "        # Convert to tensor\n",
    "        self.data = torch.tensor(encoded_data, dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]  # all but last\n",
    "        y = chunk[1:]   # all but first\n",
    "        return x, y\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.int_to_string[i.item()] for i in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H7TmswnfC1y"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1769513209405,
     "user": {
      "displayName": "Amer Delic",
      "userId": "17988139909285196208"
     },
     "user_tz": -60
    },
    "id": "TxGShNfifC1y"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int # Number of unique tokens in the vocabulary\n",
    "    block_size: int = 256 # Sequence length\n",
    "    n_block: int = 6 # Number of blocks in the transformer\n",
    "    n_head: int = 6 # Number of attention heads\n",
    "    n_embd: int = 384 # Embedding dimensionality\n",
    "    dropout: float = 0.2 # Dropout rate\n",
    "    bias: bool = True # If True, we add a bias to the LayerNorm and Linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1769513210208,
     "user": {
      "displayName": "Amer Delic",
      "userId": "17988139909285196208"
     },
     "user_tz": -60
    },
    "id": "p_2_dNObfC1y"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, f\"Embedding dimension {config.n_embd} must be divisible by number of heads {config.n_head}\"\n",
    "\n",
    "        self.n_head = config.n_head # Number of attention heads\n",
    "        self.n_embd = config.n_embd # Embedding dimensionality\n",
    "        self.dropout = config.dropout # Dropout rate\n",
    "\n",
    "        # Maps embedding into Q, K, V. We'll use one layer to generate these matrices for all heads at once.\n",
    "        # This is more efficient since everything can be computes as one single matrix multiplication.\n",
    "        self.qkv_map = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "\n",
    "        # After performing attention for each head individually, we project the results back to the embedding space.\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        # Regularization\n",
    "        self.final_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        d_k = C // self.n_head # Dimension of the query and key (within a head)\n",
    "\n",
    "        # TODO: Implement Causal Self Attention\n",
    "        # Hint: The output of the qkv_map is a tensor of shape (B, T, 3*C).\n",
    "        # We need to split this tensor into Q, K, and V tensors of shape (B, T, C) each. Split it in this precise order for the test function (see below) to work.\n",
    "        # Afterwards, reshape and transpose them to the correct shape (see assert statements),\n",
    "        # such that we have (smaller) Q, K, and V matrices for each head.\n",
    "\n",
    "        # 1. Linear projection into Q, K, V separately\n",
    "        qkv = self.qkv_map(x)  # (B, T, 3*C)\n",
    "        Q, K, V = qkv.split(C, dim=2)  # each has shape (B, T, C)\n",
    "\n",
    "        # 2. Reshape into multi-head format\n",
    "        # split the embedding dimension into n_head Ã— d_k\n",
    "        Q = Q.view(B, T, self.n_head, d_k).transpose(1, 2)  # (B, n_head, T, d_k)\n",
    "        K = K.view(B, T, self.n_head, d_k).transpose(1, 2)  # (B, n_head, T, d_k)\n",
    "        V = V.view(B, T, self.n_head, d_k).transpose(1, 2)  # (B, n_head, T, d_k)\n",
    "\n",
    "\n",
    "        for M in [Q, K, V]:\n",
    "            assert M.shape == (B, self.n_head, T, d_k), f\"Expected shape (B, self.n_head, T, d_k), but got {M.shape}\"\n",
    "\n",
    "        # TODO: Performs causally masked, multi-head scaled dot-product self-attention\n",
    "        # Compute the attention weights and aggregated values.\n",
    "        # Hint: Broadcasted matrix multiplication can be implemented using the @ operator.\n",
    "        # Hint: `torch.tril` may help you with masking the attention scores. Set values to -inf before the softmax to get a softmax output of 0.\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)  # (B, n_head, T, T)\n",
    "\n",
    "        # Apply causal mask\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device))\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax + dropout\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # (B, n_head, T, T)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        aggregated_vals = attn_weights @ V  # (B, n_head, T, d_k)\n",
    "\n",
    "        assert aggregated_vals.shape == (B, self.n_head, T, d_k), f\"Expected aggregated_vals shape (B, self.n_head, T, d_k), but got {aggregated_vals.shape}\"\n",
    "\n",
    "        # Combine all head outputs into the last dimension\n",
    "        out = aggregated_vals.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.proj(out) # This combines the outputs of all heads\n",
    "        out = self.final_dropout(out) # This is the final dropout layer\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrxjPsbefC1y"
   },
   "source": [
    "You can test your implementation of the `CausalSelfAttention` class by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1769513212910,
     "user": {
      "displayName": "Amer Delic",
      "userId": "17988139909285196208"
     },
     "user_tz": -60
    },
    "id": "xBnfx9r7fC1z",
    "outputId": "21f37ff5-2bbe-4958-949b-62c027229719"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/4w751xsx21xd9q6gkq4v_nbc0000gn/T/ipykernel_1799/4285277868.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  att_out_expected = torch.load('CausalSelfAttention_out.pt', map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = GPTConfig(vocab_size=10, block_size=8, n_block=6, n_head=6, n_embd=12, dropout=0.0, bias=True)\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "x = torch.randn(2, 8, 12).to(device)\n",
    "attention = CausalSelfAttention(config).to(device)\n",
    "att_out = attention(x)\n",
    "\n",
    "# Read expected output from file\n",
    "att_out_expected = torch.load('CausalSelfAttention_out.pt', map_location=device)\n",
    "\n",
    "assert torch.allclose(att_out, att_out_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1769513286035,
     "user": {
      "displayName": "Amer Delic",
      "userId": "17988139909285196208"
     },
     "user_tz": -60
    },
    "id": "UdL8qJIcfC1z"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # TODO: Implement the MLP\n",
    "        # It should consist of a linear layer, a GELU activation function, and a final linear layer.\n",
    "        # After the final linear layer, apply dropout with dropout rate config.dropout.\n",
    "        # The first linear layer should map from config.n_embd to 4 * config.n_embd.\n",
    "        # The second linear layer should map from 4 * config.n_embd back to config.n_embd.\n",
    "        # The linear layers should have a bias term if config.bias is True, and no bias term otherwise.\n",
    "\n",
    "        self.fc1 = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass of the MLP\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1769513288056,
     "user": {
      "displayName": "Amer Delic",
      "userId": "17988139909285196208"
     },
     "user_tz": -60
    },
    "id": "yg9xf2yefC1z"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layernorm_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attention = CausalSelfAttention(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layernorm_1(x))\n",
    "        x = x + self.mlp(self.layernorm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1769513290628,
     "user": {
      "displayName": "Amer Delic",
      "userId": "17988139909285196208"
     },
     "user_tz": -60
    },
    "id": "vOEqpqx3fC1z"
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            embed_token = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            embed_position = nn.Embedding(config.block_size, config.n_embd),\n",
    "            dropout = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_block)]),\n",
    "            layernorm = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # We use the same weights for the token embeddings and the final linear layer, since they in principal do the same thing.\n",
    "        # This can help speed up training.\n",
    "        # For more information, you can have a look at https://doi.org/10.48550/arXiv.1608.05859\n",
    "        self.transformer.embed_token.weight = self.head.weight\n",
    "\n",
    "        # Initialize all linear layers using our custom init function\n",
    "        self.apply(self._init_params)\n",
    "\n",
    "        # report number of parameters\n",
    "        print(f\"Number of parameters in GPT: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "    def _init_params(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def get_num_params(self):\n",
    "        unique_params = {id(p) : p.numel() for p in self.parameters()}\n",
    "        return sum(unique_params.values())\n",
    "\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.shape\n",
    "        assert t <= self.config.block_size, f\"Cannot process sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        position_idxs = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # TODO: Implement the forward pass of the GPT model\n",
    "        # Embed the tokens and positions using the embedding layers self.transformer.embed_token and self.transformer.embed_position.\n",
    "        # Add the token embeddings and position embeddings together and pass the result through the dropout layer.\n",
    "        # Pass the result through all the transformer blocks.\n",
    "        # Apply layer normalization and finally obtain the logits by project the result to\n",
    "        # the vocabulary space using the head layer.\n",
    "\n",
    "        # Token and position embedding\n",
    "        tok_emb = self.transformer.embed_token(idx)  # (B, T, n_embd)\n",
    "        pos_emb = self.transformer.embed_position(position_idxs)  # (T, n_embd)\n",
    "        pos_emb = pos_emb.unsqueeze(0)  # (1, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Dropout\n",
    "        x = self.transformer.dropout(x)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Final layer norm\n",
    "        x = self.transformer.layernorm(x)\n",
    "\n",
    "        # Output head\n",
    "        # logits = self.transformer.head(x) # (B, T, vocab_size)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def get_optimizer(self, weight_decay, learning_rate, betas, device):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "\n",
    "        # We will decay all parameters that are 2D or higher dimensional.\n",
    "        # This includes all weight matrices and embeddings.\n",
    "        decay_params = [p for n, p in param_dict.items() if len(p.shape) >= 2]\n",
    "        # We will not decay biases and layernorm parameters (which are 1D).\n",
    "        nodecay_params = [p for n, p in param_dict.items() if len(p.shape) < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        fused = (device == 'cuda')\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=fused)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is of shape (batch_size, sequence_length)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # If the sequence context is growing too long we must crop it at block_size\n",
    "            idx_input = idx if idx.shape[1] <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # TODO: Push idx_input through the model to get the logits for the next token in the sequence\n",
    "            # Hint: The logits that are returned by the model are of shape (batch_size, sequence_length, vocab_size).\n",
    "            # To predict the next token, we only need the logits for the last position in the sequence.\n",
    "            # Next, divide the logits by the desired temperature and apply the softmax function to convert them to probabilities.\n",
    "            # Finally, sample the next token from this probability distribution.\n",
    "\n",
    "            # Forward pass\n",
    "            logits = self(idx_input)  # (B, T, vocab_size)\n",
    "\n",
    "            # Extract logits for the last position and scale\n",
    "            logits = logits[:, -1, :] / temperature  # (B, vocab_size)\n",
    "\n",
    "            # Convert scaled logits into a probability distribution\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
    "\n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            assert next_token.shape == (idx.shape[0], 1), f\"Expected next_token shape (batch_size, 1), but got {next_token.shape}\"\n",
    "\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipJprylZfC10"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "G69GBnA8fC10"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_train_val_loss(model, train_loader, val_loader, val_iters, device):\n",
    "    model.eval()\n",
    "    losses = {}\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        total_loss = 0\n",
    "        for i, (X, Y) in enumerate(loader):\n",
    "            if i >= val_iters:\n",
    "                break\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), Y.view(-1), ignore_index=-1)\n",
    "            total_loss += loss.item()\n",
    "        losses[split] = total_loss / val_iters\n",
    "    model.train()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9BSRS9NSfC10"
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 128\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "data = MovieDataset(block_size)\n",
    "\n",
    "# split into train and validation sets\n",
    "train_len = int(len(data) * 0.8)\n",
    "val_len = len(data) - train_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(data, [train_len, val_len])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# TODO: You may fetch a batch from the dataloader here to answer Task a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "O9G2pP7dfC10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT: 4.80M\n",
      "Number of parameters in GPT: 0.83M\n",
      "iter 0: loss 5.1928\n",
      "step 0: train loss 4.7834, val loss 4.7831\n",
      "iter 10: loss 3.5900\n",
      "iter 20: loss 3.0305\n",
      "iter 30: loss 2.7852\n",
      "iter 40: loss 2.6509\n",
      "iter 50: loss 2.6117\n",
      "iter 60: loss 2.5800\n",
      "iter 70: loss 2.5489\n",
      "iter 80: loss 2.5632\n",
      "iter 90: loss 2.5293\n",
      "iter 100: loss 2.5195\n",
      "iter 110: loss 2.5032\n",
      "iter 120: loss 2.5146\n",
      "iter 130: loss 2.4866\n",
      "iter 140: loss 2.4731\n",
      "iter 150: loss 2.4590\n",
      "iter 160: loss 2.4507\n",
      "iter 170: loss 2.4216\n",
      "iter 180: loss 2.4202\n",
      "iter 190: loss 2.3842\n",
      "iter 200: loss 2.3521\n",
      "iter 210: loss 2.3568\n",
      "iter 220: loss 2.3175\n",
      "iter 230: loss 2.3030\n",
      "iter 240: loss 2.2716\n",
      "iter 250: loss 2.2505\n",
      "iter 260: loss 2.2404\n",
      "iter 270: loss 2.1897\n",
      "iter 280: loss 2.1513\n",
      "iter 290: loss 2.1485\n",
      "iter 300: loss 2.1433\n",
      "iter 310: loss 2.1431\n",
      "iter 320: loss 2.1372\n",
      "iter 330: loss 2.0918\n",
      "iter 340: loss 2.0819\n",
      "iter 350: loss 2.0723\n",
      "iter 360: loss 2.0573\n",
      "iter 370: loss 2.0393\n",
      "iter 380: loss 2.0456\n",
      "iter 390: loss 2.0257\n",
      "iter 400: loss 1.9839\n",
      "iter 410: loss 1.9695\n",
      "iter 420: loss 1.9688\n",
      "iter 430: loss 1.9618\n",
      "iter 440: loss 1.9313\n",
      "iter 450: loss 1.9040\n",
      "iter 460: loss 1.9015\n",
      "iter 470: loss 1.8868\n",
      "iter 480: loss 1.9035\n",
      "iter 490: loss 1.8616\n",
      "iter 500: loss 1.8973\n",
      "step 500: train loss 1.8782, val loss 1.8872\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 510: loss 1.8693\n",
      "iter 520: loss 1.8769\n",
      "iter 530: loss 1.8648\n",
      "iter 540: loss 1.8314\n",
      "iter 550: loss 1.8254\n",
      "iter 560: loss 1.8397\n",
      "iter 570: loss 1.8225\n",
      "iter 580: loss 1.8023\n",
      "iter 590: loss 1.7914\n",
      "iter 600: loss 1.7715\n",
      "iter 610: loss 1.7831\n",
      "iter 620: loss 1.7607\n",
      "iter 630: loss 1.7323\n",
      "iter 640: loss 1.7447\n",
      "iter 650: loss 1.7619\n",
      "iter 660: loss 1.7162\n",
      "iter 670: loss 1.7203\n",
      "iter 680: loss 1.7475\n",
      "iter 690: loss 1.7289\n",
      "iter 700: loss 1.7209\n",
      "iter 710: loss 1.7423\n",
      "iter 720: loss 1.6842\n",
      "iter 730: loss 1.7089\n",
      "iter 740: loss 1.7080\n",
      "iter 750: loss 1.6869\n",
      "iter 760: loss 1.6879\n",
      "iter 770: loss 1.6771\n",
      "iter 780: loss 1.6609\n",
      "iter 790: loss 1.6585\n",
      "iter 800: loss 1.6531\n",
      "iter 810: loss 1.6728\n",
      "iter 820: loss 1.6626\n",
      "iter 830: loss 1.6185\n",
      "iter 840: loss 1.6665\n",
      "iter 850: loss 1.6293\n",
      "iter 860: loss 1.6338\n",
      "iter 870: loss 1.6279\n",
      "iter 880: loss 1.6038\n",
      "iter 890: loss 1.6490\n",
      "iter 900: loss 1.5854\n",
      "iter 910: loss 1.6050\n",
      "iter 920: loss 1.5705\n",
      "iter 930: loss 1.6326\n",
      "iter 940: loss 1.5695\n",
      "iter 950: loss 1.5877\n",
      "iter 960: loss 1.6066\n",
      "iter 970: loss 1.5495\n",
      "iter 980: loss 1.6164\n",
      "iter 990: loss 1.5742\n",
      "iter 1000: loss 1.5707\n",
      "step 1000: train loss 1.5631, val loss 1.5727\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 1010: loss 1.5543\n",
      "iter 1020: loss 1.5730\n",
      "iter 1030: loss 1.5610\n",
      "iter 1040: loss 1.5826\n",
      "iter 1050: loss 1.5020\n",
      "iter 1060: loss 1.5612\n",
      "iter 1070: loss 1.5533\n",
      "iter 1080: loss 1.5632\n",
      "iter 1090: loss 1.5070\n",
      "iter 1100: loss 1.5225\n",
      "iter 1110: loss 1.5433\n",
      "iter 1120: loss 1.5043\n",
      "iter 1130: loss 1.5221\n",
      "iter 1140: loss 1.4992\n",
      "iter 1150: loss 1.4875\n",
      "iter 1160: loss 1.5320\n",
      "iter 1170: loss 1.5415\n",
      "iter 1180: loss 1.5090\n",
      "iter 1190: loss 1.5029\n",
      "iter 1200: loss 1.5195\n",
      "iter 1210: loss 1.4874\n",
      "iter 1220: loss 1.4836\n",
      "iter 1230: loss 1.5283\n",
      "iter 1240: loss 1.4915\n",
      "iter 1250: loss 1.4785\n",
      "iter 1260: loss 1.4991\n",
      "iter 1270: loss 1.4992\n",
      "iter 1280: loss 1.5215\n",
      "iter 1290: loss 1.4872\n",
      "iter 1300: loss 1.4685\n",
      "iter 1310: loss 1.4909\n",
      "iter 1320: loss 1.4516\n",
      "iter 1330: loss 1.4561\n",
      "iter 1340: loss 1.4647\n",
      "iter 1350: loss 1.4498\n",
      "iter 1360: loss 1.4744\n",
      "iter 1370: loss 1.4579\n",
      "iter 1380: loss 1.4550\n",
      "iter 1390: loss 1.4545\n",
      "iter 1400: loss 1.4591\n",
      "iter 1410: loss 1.4574\n",
      "iter 1420: loss 1.4597\n",
      "iter 1430: loss 1.4330\n",
      "iter 1440: loss 1.4685\n",
      "iter 1450: loss 1.4520\n",
      "iter 1460: loss 1.4359\n",
      "iter 1470: loss 1.4300\n",
      "iter 1480: loss 1.4745\n",
      "iter 1490: loss 1.4623\n",
      "iter 1500: loss 1.4432\n",
      "step 1500: train loss 1.4299, val loss 1.4410\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 1510: loss 1.4475\n",
      "iter 1520: loss 1.4088\n",
      "iter 1530: loss 1.4540\n",
      "iter 1540: loss 1.4167\n",
      "iter 1550: loss 1.4472\n",
      "iter 1560: loss 1.4328\n",
      "iter 1570: loss 1.4471\n",
      "iter 1580: loss 1.4374\n",
      "iter 1590: loss 1.4459\n",
      "iter 1600: loss 1.4093\n",
      "iter 1610: loss 1.4692\n",
      "iter 1620: loss 1.4430\n",
      "iter 1630: loss 1.4208\n",
      "iter 1640: loss 1.4258\n",
      "iter 1650: loss 1.3881\n",
      "iter 1660: loss 1.3909\n",
      "iter 1670: loss 1.3907\n",
      "iter 1680: loss 1.4377\n",
      "iter 1690: loss 1.3841\n",
      "iter 1700: loss 1.4130\n",
      "iter 1710: loss 1.4007\n",
      "iter 1720: loss 1.4046\n",
      "iter 1730: loss 1.4023\n",
      "iter 1740: loss 1.4347\n",
      "iter 1750: loss 1.4395\n",
      "iter 1760: loss 1.4353\n",
      "iter 1770: loss 1.4209\n",
      "iter 1780: loss 1.4260\n",
      "iter 1790: loss 1.3955\n",
      "iter 1800: loss 1.3961\n",
      "iter 1810: loss 1.4127\n",
      "iter 1820: loss 1.3877\n",
      "iter 1830: loss 1.3931\n",
      "iter 1840: loss 1.3957\n",
      "iter 1850: loss 1.3847\n",
      "iter 1860: loss 1.3978\n",
      "iter 1870: loss 1.3733\n",
      "iter 1880: loss 1.4057\n",
      "iter 1890: loss 1.3965\n",
      "iter 1900: loss 1.3899\n",
      "iter 1910: loss 1.3761\n",
      "iter 1920: loss 1.3654\n",
      "iter 1930: loss 1.3685\n",
      "iter 1940: loss 1.3712\n",
      "iter 1950: loss 1.3614\n",
      "iter 1960: loss 1.3770\n",
      "iter 1970: loss 1.3681\n",
      "iter 1980: loss 1.3673\n",
      "iter 1990: loss 1.3804\n",
      "iter 2000: loss 1.3759\n",
      "step 2000: train loss 1.3791, val loss 1.3820\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 2010: loss 1.3839\n",
      "iter 2020: loss 1.3618\n",
      "iter 2030: loss 1.3786\n",
      "iter 2040: loss 1.4029\n",
      "iter 2050: loss 1.3927\n",
      "iter 2060: loss 1.3804\n",
      "iter 2070: loss 1.3519\n",
      "iter 2080: loss 1.3427\n",
      "iter 2090: loss 1.3790\n",
      "iter 2100: loss 1.3512\n",
      "iter 2110: loss 1.3570\n",
      "iter 2120: loss 1.3642\n",
      "iter 2130: loss 1.3702\n",
      "iter 2140: loss 1.3396\n",
      "iter 2150: loss 1.3346\n",
      "iter 2160: loss 1.3497\n",
      "iter 2170: loss 1.3493\n",
      "iter 2180: loss 1.3574\n",
      "iter 2190: loss 1.3677\n",
      "iter 2200: loss 1.3479\n",
      "iter 2210: loss 1.3531\n",
      "iter 2220: loss 1.3742\n",
      "iter 2230: loss 1.3481\n",
      "iter 2240: loss 1.3782\n",
      "iter 2250: loss 1.3447\n",
      "iter 2260: loss 1.3234\n",
      "iter 2270: loss 1.3238\n",
      "iter 2280: loss 1.3534\n",
      "iter 2290: loss 1.3580\n",
      "iter 2300: loss 1.3561\n",
      "iter 2310: loss 1.3420\n",
      "iter 2320: loss 1.3729\n",
      "iter 2330: loss 1.3364\n",
      "iter 2340: loss 1.3193\n",
      "iter 2350: loss 1.3428\n",
      "iter 2360: loss 1.3278\n",
      "iter 2370: loss 1.3442\n",
      "iter 2380: loss 1.3654\n",
      "iter 2390: loss 1.3356\n",
      "iter 2400: loss 1.3410\n",
      "iter 2410: loss 1.3531\n",
      "iter 2420: loss 1.3553\n",
      "iter 2430: loss 1.3154\n",
      "iter 2440: loss 1.3608\n",
      "iter 2450: loss 1.3522\n",
      "iter 2460: loss 1.3242\n",
      "iter 2470: loss 1.3391\n",
      "iter 2480: loss 1.3271\n",
      "iter 2490: loss 1.3441\n",
      "iter 2500: loss 1.3377\n",
      "step 2500: train loss 1.3256, val loss 1.3391\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 2510: loss 1.3577\n",
      "iter 2520: loss 1.3153\n",
      "iter 2530: loss 1.3562\n",
      "iter 2540: loss 1.3308\n",
      "iter 2550: loss 1.3541\n",
      "iter 2560: loss 1.3268\n",
      "iter 2570: loss 1.3199\n",
      "iter 2580: loss 1.3052\n",
      "iter 2590: loss 1.3297\n",
      "iter 2600: loss 1.3415\n",
      "iter 2610: loss 1.3266\n",
      "iter 2620: loss 1.3297\n",
      "iter 2630: loss 1.3376\n",
      "iter 2640: loss 1.3110\n",
      "iter 2650: loss 1.3294\n",
      "iter 2660: loss 1.2973\n",
      "iter 2670: loss 1.3162\n",
      "iter 2680: loss 1.2995\n",
      "iter 2690: loss 1.3254\n",
      "iter 2700: loss 1.3180\n",
      "iter 2710: loss 1.3248\n",
      "iter 2720: loss 1.2992\n",
      "iter 2730: loss 1.3363\n",
      "iter 2740: loss 1.3106\n",
      "iter 2750: loss 1.3080\n",
      "iter 2760: loss 1.3007\n",
      "iter 2770: loss 1.3344\n",
      "iter 2780: loss 1.3029\n",
      "iter 2790: loss 1.3048\n",
      "iter 2800: loss 1.2945\n",
      "iter 2810: loss 1.3080\n",
      "iter 2820: loss 1.2992\n",
      "iter 2830: loss 1.3055\n",
      "iter 2840: loss 1.2942\n",
      "iter 2850: loss 1.3336\n",
      "iter 2860: loss 1.3272\n",
      "iter 2870: loss 1.3387\n",
      "iter 2880: loss 1.2940\n",
      "iter 2890: loss 1.3318\n",
      "iter 2900: loss 1.2964\n",
      "iter 2910: loss 1.3177\n",
      "iter 2920: loss 1.3226\n",
      "iter 2930: loss 1.2951\n",
      "iter 2940: loss 1.2790\n",
      "iter 2950: loss 1.3209\n",
      "iter 2960: loss 1.3213\n",
      "iter 2970: loss 1.2950\n",
      "iter 2980: loss 1.2885\n",
      "iter 2990: loss 1.3218\n",
      "iter 3000: loss 1.3007\n",
      "step 3000: train loss 1.3109, val loss 1.3122\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 3010: loss 1.2939\n",
      "iter 3020: loss 1.3094\n",
      "iter 3030: loss 1.2919\n",
      "iter 3040: loss 1.2784\n",
      "iter 3050: loss 1.3251\n",
      "iter 3060: loss 1.2753\n",
      "iter 3070: loss 1.2945\n",
      "iter 3080: loss 1.3344\n",
      "iter 3090: loss 1.2968\n",
      "iter 3100: loss 1.3307\n",
      "iter 3110: loss 1.2908\n",
      "iter 3120: loss 1.3008\n",
      "iter 3130: loss 1.2869\n",
      "iter 3140: loss 1.2881\n",
      "iter 3150: loss 1.2885\n",
      "iter 3160: loss 1.2926\n",
      "iter 3170: loss 1.3085\n",
      "iter 3180: loss 1.3245\n",
      "iter 3190: loss 1.2782\n",
      "iter 3200: loss 1.2855\n",
      "iter 3210: loss 1.2974\n",
      "iter 3220: loss 1.2971\n",
      "iter 3230: loss 1.2949\n",
      "iter 3240: loss 1.3057\n",
      "iter 3250: loss 1.3118\n",
      "iter 3260: loss 1.3097\n",
      "iter 3270: loss 1.3177\n",
      "iter 3280: loss 1.3094\n",
      "iter 3290: loss 1.2738\n",
      "iter 3300: loss 1.3076\n",
      "iter 3310: loss 1.2997\n",
      "iter 3320: loss 1.3014\n",
      "iter 3330: loss 1.2490\n",
      "iter 3340: loss 1.2786\n",
      "iter 3350: loss 1.2804\n",
      "iter 3360: loss 1.2726\n",
      "iter 3370: loss 1.2973\n",
      "iter 3380: loss 1.2753\n",
      "iter 3390: loss 1.2868\n",
      "iter 3400: loss 1.2986\n",
      "iter 3410: loss 1.2784\n",
      "iter 3420: loss 1.3096\n",
      "iter 3430: loss 1.2712\n",
      "iter 3440: loss 1.2868\n",
      "iter 3450: loss 1.2614\n",
      "iter 3460: loss 1.2545\n",
      "iter 3470: loss 1.2789\n",
      "iter 3480: loss 1.2702\n",
      "iter 3490: loss 1.2812\n",
      "iter 3500: loss 1.2867\n",
      "step 3500: train loss 1.2867, val loss 1.2894\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 3510: loss 1.2978\n",
      "iter 3520: loss 1.2973\n",
      "iter 3530: loss 1.2957\n",
      "iter 3540: loss 1.2827\n",
      "iter 3550: loss 1.3279\n",
      "iter 3560: loss 1.2678\n",
      "iter 3570: loss 1.2710\n",
      "iter 3580: loss 1.2766\n",
      "iter 3590: loss 1.2890\n",
      "iter 3600: loss 1.3056\n",
      "iter 3610: loss 1.2755\n",
      "iter 3620: loss 1.2782\n",
      "iter 3630: loss 1.2869\n",
      "iter 3640: loss 1.2627\n",
      "iter 3650: loss 1.2638\n",
      "iter 3660: loss 1.2773\n",
      "iter 3670: loss 1.2676\n",
      "iter 3680: loss 1.2792\n",
      "iter 3690: loss 1.2903\n",
      "iter 3700: loss 1.2890\n",
      "iter 3710: loss 1.2847\n",
      "iter 3720: loss 1.2570\n",
      "iter 3730: loss 1.2726\n",
      "iter 3740: loss 1.2929\n",
      "iter 3750: loss 1.2705\n",
      "iter 3760: loss 1.2754\n",
      "iter 3770: loss 1.3116\n",
      "iter 3780: loss 1.2851\n",
      "iter 3790: loss 1.2462\n",
      "iter 3800: loss 1.2706\n",
      "iter 3810: loss 1.2625\n",
      "iter 3820: loss 1.2479\n",
      "iter 3830: loss 1.2786\n",
      "iter 3840: loss 1.2798\n",
      "iter 3850: loss 1.2967\n",
      "iter 3860: loss 1.2498\n",
      "iter 3870: loss 1.2701\n",
      "iter 3880: loss 1.2656\n",
      "iter 3890: loss 1.2831\n",
      "iter 3900: loss 1.2724\n",
      "iter 3910: loss 1.2572\n",
      "iter 3920: loss 1.2840\n",
      "iter 3930: loss 1.2703\n",
      "iter 3940: loss 1.2391\n",
      "iter 3950: loss 1.2463\n",
      "iter 3960: loss 1.2529\n",
      "iter 3970: loss 1.2950\n",
      "iter 3980: loss 1.2659\n",
      "iter 3990: loss 1.2749\n",
      "iter 4000: loss 1.2584\n",
      "step 4000: train loss 1.2606, val loss 1.2701\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 4010: loss 1.2622\n",
      "iter 4020: loss 1.2658\n",
      "iter 4030: loss 1.2642\n",
      "iter 4040: loss 1.2693\n",
      "iter 4050: loss 1.2505\n",
      "iter 4060: loss 1.2557\n",
      "iter 4070: loss 1.2657\n",
      "iter 4080: loss 1.2656\n",
      "iter 4090: loss 1.2738\n",
      "iter 4100: loss 1.2382\n",
      "iter 4110: loss 1.2574\n",
      "iter 4120: loss 1.2803\n",
      "iter 4130: loss 1.2406\n",
      "iter 4140: loss 1.2528\n",
      "iter 4150: loss 1.2879\n",
      "iter 4160: loss 1.2342\n",
      "iter 4170: loss 1.2327\n",
      "iter 4180: loss 1.2679\n",
      "iter 4190: loss 1.2270\n",
      "iter 4200: loss 1.2820\n",
      "iter 4210: loss 1.2459\n",
      "iter 4220: loss 1.2550\n",
      "iter 4230: loss 1.2460\n",
      "iter 4240: loss 1.2627\n",
      "iter 4250: loss 1.2753\n",
      "iter 4260: loss 1.2321\n",
      "iter 4270: loss 1.2694\n",
      "iter 4280: loss 1.2455\n",
      "iter 4290: loss 1.2605\n",
      "iter 4300: loss 1.2600\n",
      "iter 4310: loss 1.2479\n",
      "iter 4320: loss 1.2538\n",
      "iter 4330: loss 1.2787\n",
      "iter 4340: loss 1.2401\n",
      "iter 4350: loss 1.2499\n",
      "iter 4360: loss 1.2666\n",
      "iter 4370: loss 1.2420\n",
      "iter 4380: loss 1.2733\n",
      "iter 4390: loss 1.2669\n",
      "iter 4400: loss 1.2841\n",
      "iter 4410: loss 1.2415\n",
      "iter 4420: loss 1.2480\n",
      "iter 4430: loss 1.2637\n",
      "iter 4440: loss 1.2461\n",
      "iter 4450: loss 1.2646\n",
      "iter 4460: loss 1.2519\n",
      "iter 4470: loss 1.2413\n",
      "iter 4480: loss 1.2545\n",
      "iter 4490: loss 1.2418\n",
      "iter 4500: loss 1.2458\n",
      "step 4500: train loss 1.2428, val loss 1.2518\n",
      "Saving checkpoint to MovieGPT\n",
      "iter 4510: loss 1.2426\n",
      "iter 4520: loss 1.2558\n",
      "iter 4530: loss 1.2580\n",
      "iter 4540: loss 1.2596\n",
      "iter 4550: loss 1.2466\n",
      "iter 4560: loss 1.2608\n",
      "iter 4570: loss 1.2695\n",
      "iter 4580: loss 1.2409\n",
      "iter 4590: loss 1.2472\n",
      "iter 4600: loss 1.2326\n",
      "iter 4610: loss 1.2395\n",
      "iter 4620: loss 1.2285\n",
      "iter 4630: loss 1.2693\n",
      "iter 4640: loss 1.2483\n",
      "iter 4650: loss 1.2428\n",
      "iter 4660: loss 1.2396\n",
      "iter 4670: loss 1.2239\n",
      "iter 4680: loss 1.2381\n",
      "iter 4690: loss 1.2455\n",
      "iter 4700: loss 1.2350\n",
      "iter 4710: loss 1.2544\n",
      "iter 4720: loss 1.2391\n",
      "iter 4730: loss 1.2272\n",
      "iter 4740: loss 1.2609\n",
      "iter 4750: loss 1.2428\n",
      "iter 4760: loss 1.2297\n",
      "iter 4770: loss 1.2545\n",
      "iter 4780: loss 1.2368\n",
      "iter 4790: loss 1.2276\n",
      "iter 4800: loss 1.2293\n",
      "iter 4810: loss 1.2643\n",
      "iter 4820: loss 1.2309\n",
      "iter 4830: loss 1.2625\n",
      "iter 4840: loss 1.2323\n",
      "iter 4850: loss 1.2242\n",
      "iter 4860: loss 1.2690\n",
      "iter 4870: loss 1.2394\n",
      "iter 4880: loss 1.2535\n",
      "iter 4890: loss 1.2370\n",
      "iter 4900: loss 1.2826\n",
      "iter 4910: loss 1.2219\n",
      "iter 4920: loss 1.2472\n",
      "iter 4930: loss 1.2170\n",
      "iter 4940: loss 1.2879\n",
      "iter 4950: loss 1.2310\n",
      "iter 4960: loss 1.2315\n",
      "iter 4970: loss 1.2280\n",
      "iter 4980: loss 1.2199\n",
      "iter 4990: loss 1.2350\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "out_dir = 'MovieGPT'\n",
    "checkpoint_path = os.path.join(out_dir, 'checkpoint.pt')\n",
    "os.makedirs(out_dir, exist_ok=True)  # Create output directory\n",
    "\n",
    "# TODO: You may change the training configuration as desired, especially in Task h)\n",
    "# You may also add loops for automated hyperparameter search and\n",
    "# timestamps to track training time.\n",
    "\n",
    "# Eval/Logging\n",
    "val_interval = 500 # Number of iterations between evaluations\n",
    "val_iters = 20 # Number of iterations for evaluation\n",
    "log_interval = 10 # Number of iterations between logging\n",
    "\n",
    "# Optimizer settings\n",
    "learning_rate = 1e-3 # Larger networks typically require a learning rate that is smaller than this\n",
    "max_iters = 5_000 # Number of iterations to train for\n",
    "weight_decay = 1e-1 # Weight decay for regularization (on the weights/embeddings)\n",
    "beta1, beta2 = 0.9, 0.99 # Beta1, Beta2 for AdamW optimizer\n",
    "grad_clip = 1.0 # Clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# Compile model\n",
    "compile_model = False # Compile the model for faster execution. Note: no debug breakpoints in compiled models!\n",
    "\n",
    "# Model config\n",
    "vocab_size = len(data.string_to_int)\n",
    "config = GPTConfig(\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_block=4,\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    dropout=0.0,\n",
    "    bias=False\n",
    ") # This is a relatively small model\n",
    "\n",
    "# ===== Task i) START: Larger model =====\n",
    "\n",
    "config_large = GPTConfig(\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_block=6,\n",
    "    n_head=8,\n",
    "    n_embd=256,\n",
    "    dropout=0.0,\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "model_large = GPT(config_large).to(device)\n",
    "\n",
    "optimizer_large = model_large.get_optimizer(\n",
    "    weight_decay, learning_rate, (beta1, beta2), device\n",
    ")\n",
    "\n",
    "# ===== Task i) END: Larger model =====\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "\n",
    "if compile_model:\n",
    "    print(\"Compiling the model...\")\n",
    "    model = torch.compile(model) # Needs PyTorch >= 2.0\n",
    "    print(\"Done compiling\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = model.get_optimizer(weight_decay, learning_rate, (beta1, beta2), device)\n",
    "\n",
    "# Training loop\n",
    "iter_num = 0\n",
    "best_val_loss = float('inf')\n",
    "eval_steps = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for _ in range(max_iters):\n",
    "    for X, Y in train_loader:\n",
    "        # Get batch and move to device\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(X)\n",
    "\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), Y.view(-1), ignore_index=-1)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        if grad_clip != 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if iter_num % log_interval == 0:\n",
    "            print(f\"iter {iter_num}: loss {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluation\n",
    "        if iter_num % val_interval == 0:\n",
    "            losses = estimate_train_val_loss(model, train_loader, val_loader, val_iters, device)\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "            eval_steps.append(iter_num)\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "\n",
    "            # Save best model\n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    print(f\"Saving checkpoint to {out_dir}\")\n",
    "                    model_to_save = model._orig_mod if compile_model else model\n",
    "                    torch.save({\n",
    "                        'model': model_to_save.state_dict(),\n",
    "                        'model_args': config,\n",
    "                    }, checkpoint_path)\n",
    "\n",
    "        iter_num += 1\n",
    "        if iter_num >= max_iters:\n",
    "            break\n",
    "\n",
    "    if iter_num >= max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "aqgwhoQ7Kg1B"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Plot the train- and validation loss.\n",
    "# To speed up training, they are only computed every val_interval iterations. Adapt this value if you like.\n",
    "# Since dropout is enabled during training, the training loss is computed again during the validation step without dropout.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(eval_steps, train_losses, label=\"Train loss\")\n",
    "plt.plot(eval_steps, val_losses, label=\"Validation loss\")\n",
    "plt.xlabel(\"Tterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig(\"figures/train_val_loss.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP0y37aRfC11"
   },
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8JlLc9vKfC11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/4w751xsx21xd9q6gkq4v_nbc0000gn/T/ipykernel_1799/2398746619.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT: 0.83M\n",
      "--------------------\n",
      "\n",
      "Beautiful New York Secret: Target: In 1979 life teenage's meddits to leave his new game in the school. But the most dance full as Michean's roading friends most place where teensions must set to become the money test, where high sconnizing respection who throws have to talk her to save him a provicative story in the commandant and encounters the last rising and murder his own.\n",
      "Sport: Grimmber Band Rambo: A man who lives in attempt to find the boy childrogy back her, elderly plans to get a quit w\n",
      "--------------------\n",
      "\n",
      "The Police Door: Two former champion Sara: A team of supply people and Dark Federative artists having abductions, a music teenage genetatival, a vampire broker John Hawk and her hotel they mustangle to pay on a pright, she turns to graces the pressure hero grand people what the commandant becomes a quiet thereâ€™s facility and under the year of a property road to police crazy working from her back in the fall of an insurance company and wind going for enemy.\n",
      "Flash Humb: A sudden upside-car-based s\n",
      "--------------------\n",
      "\n",
      "EPOB IR: A peaceful wide on a storm flooding attempt to hunt the classe, Andrew is selecting to fright him to combat may the truth.\n",
      "Fight Barrel: Detective driven all the year worker and expectedly wave of letters these year old boy, this rest off in a metal companiot when she realizes Carlo and Harlem become now suffers a mortal crippleted strong by Gaulf tough-oring and lives. When he encounters a deal where she wonders into a few billion to victory to rescue a from conting who share her lover\n",
      "--------------------\n",
      "\n",
      "C: A dorman-depressional mob when the New York City faces a series of Petagins and a semble love, since of his community is tormented to little. Unfortunately, the test of the film and intime to the courth-on-to-o-scenese, including technique is coming -- to save Sparkle and her two film love and get up rescue on an isolated part terrorists of the closest of the rest of her husband's family becomes a free Protrotic computen and falls into a lone long-landing at the hands of their new during in t\n",
      "--------------------\n",
      "\n",
      "The Village: Alien who was asked to do the one-before he made when his own conflict and soldiers are everything for the orthess and one another she dies.  Shirlo and his heart takes a chance to an ex-with a color.\n",
      "Predage Truth: Sky: Barry Mouantain Captain America, to make a journey through the trouble debust head into a lovable and powerful father Rooks, and Church's strength to Shin in the true story of Ripley what he is challenged to say place to get on the future, St. Tarzak, a futuous gay \n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "num_samples = 5  # Number of samples to draw\n",
    "max_new_tokens = 500  # Number of tokens generated in each sample.\n",
    "temperature = 0.8  # TODO: Use different temperature values and qualitatively report on the results\n",
    "# temperature = 0.2\n",
    "# temperature = 1.4\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 345\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "config = checkpoint['model_args']\n",
    "model = GPT(config)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Create dataset to get encoder/decoder\n",
    "dataset = MovieDataset(block_size=config.block_size)\n",
    "encode = lambda s: [dataset.string_to_int[c] for c in s]\n",
    "decode = dataset.decode\n",
    "\n",
    "# TODO: You may change the loop for automated testing\n",
    "# Generate samples\n",
    "print('-'*20)\n",
    "with torch.no_grad():\n",
    "    for k in range(num_samples):\n",
    "        start_prompt = \"\\n\"  # TODO: Use different start prompts and compare the results\n",
    "        # start_prompt = \"Once in a land, far, far, away...\"\n",
    "        # start_prompt = \"A group of friends\"\n",
    "        # start_prompt = \"A young man discovers\"\n",
    "        prompt_ids = encode(start_prompt)\n",
    "\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n",
    "        y = model.sample(x, max_new_tokens, temperature=temperature)\n",
    "        print(decode(y[0]))\n",
    "        print('-'*20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
